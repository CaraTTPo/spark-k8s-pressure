apiVersion: v1
data:
  spark.properties: |
    #Java properties built from Kubernetes config map with name: {appname}-conf-map
    #Tue Sep 10 10:55:00 CST 2019
    spark.driver.port=7078
    spark.kubernetes.resource.type=python
    spark.executor.extraLibraryPath=/usr/hdp/current/hadoop-client/lib/native\:/usr/hdp/current/hadoop-client/lib/native/Linux-amd64-64
    spark.kubernetes.executor.volumes.persistentVolumeClaim.log.options.claimName=spark-dev-log
    spark.kubernetes.executor.volumes.persistentVolumeClaim.log.mount.path=/srv/log
    spark.kubernetes.executor.volumes.persistentVolumeClaim.log.mount.readOnly=false
    spark.kubernetes.driver.volumes.persistentVolumeClaim.log.options.claimName=spark-dev-log
    spark.kubernetes.driver.volumes.persistentVolumeClaim.log.mount.path=/srv/log
    spark.kubernetes.driver.volumes.persistentVolumeClaim.log.mount.readOnly=false
    spark.kubernetes.python.mainAppResource=hdfs\:///user/hadoop/etlsdk/run.py
    spark.ui.enabled=false
    spark.sql.ui.retainedExecutions=10
    spark.files=hdfs\:///user/hadoop/etlsdk/run.py
    spark.history.provider=org.apache.spark.deploy.history.FsHistoryProvider
    spark.kubernetes.container.image=registry-vpc.cn-hangzhou.aliyuncs.com/eigenlab/data_pipeline\:test_conf
    spark.master=k8s\://https\://api.k8s.aipp.io\:6443
    spark.history.kerberos.keytab=none
    spark.kubernetes.driver.pod.name={appname}
    spark.driver.host={appname}-svc.spark-dev.svc
    spark.history.ui.port=18081
    spark.driver.extraLibraryPath=/usr/hdp/current/hadoop-client/lib/native\:/usr/hdp/current/hadoop-client/lib/native/Linux-amd64-64
    spark.shuffle.io.preferDirectBufs=false
    spark.submit.deployMode=cluster
    spark.executor.extraJavaOptions=-XX\:MaxDirectMemorySize\=512m
    spark.kubernetes.authenticate.driver.serviceAccountName=spark
    spark.kubernetes.executor.podNamePrefix={appname}
    spark.kubernetes.node.selector.env=spark-dev
    spark.kubernetes.node.selector.role=spark
    spark.history.fs.logDirectory=hdfs\:///spark2-history/
    spark.kubernetes.submitInDriver=true
    spark.kubernetes.pyspark.pythonVersion=3
    spark.kubernetes.memoryOverheadFactor=0.2
    spark.app.name={appname}
    spark.eventLog.enabled=true
    spark.kubernetes.container.image.pullPolicy=Always
    spark.driver.blockManager.port=7079
    spark.executor.memoryOverhead=1536m
    spark.driver.memory=2048m
    spark.app.id={appid}
    spark.sql.warehouse.dir=hdfs\:///apps/hive/warehouse/
    spark.eventLog.dir=hdfs\:///spark2-history/
    spark.kubernetes.container.image.pullSecrets=aliyun-registry
    spark.driver.extraClassPath=/etc/spark2/conf
    spark.executor.extraClassPath=/etc/spark2/conf
    spark.history.kerberos.principal=none
    spark.kubernetes.namespace=spark-dev
    spark.executor.instances=1
    spark.jars=hdfs\://emr2-header-1.ipa.aidigger.com\:8020/spark_jars/etlsdk/prod/mysql.jar
kind: ConfigMap
metadata:
  name: {appname}-conf-map
  namespace: spark-dev
  ownerReferences:
  - apiVersion: v1
    controller: true
    kind: Pod
    name: {appname}
    uid: {uid}
  selfLink: /api/v1/namespaces/spark-dev/configmaps/{appname}-conf-map
