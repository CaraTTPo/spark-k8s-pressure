apiVersion: v1
data:
  spark.properties: |
    #Java properties built from Kubernetes config map with name: data-pipeline-test-1569411807621-driver-conf-map
    #Wed Sep 25 19:43:28 CST 2019
    spark.driver.port=7078
    spark.kubernetes.resource.type=python
    spark.executor.extraLibraryPath=/usr/hdp/current/hadoop-client/lib/native\:/usr/hdp/current/hadoop-client/lib/native/Linux-amd64-64
    spark.kubernetes.executor.volumes.persistentVolumeClaim.log.options.claimName=spark-dev-log
    spark.kubernetes.python.mainAppResource=hdfs\:///user/hadoop/etlsdk/run.py
    spark.ui.enabled=false
    spark.kubernetes.executor.volumes.persistentVolumeClaim.log.mount.path=/tmp/log
    spark.files=hdfs\:///user/hadoop/etlsdk/run.py
    spark.history.provider=org.apache.spark.deploy.history.FsHistoryProvider
    spark.kubernetes.driver.volumes.persistentVolumeClaim.log.mount.readOnly=false
    spark.executor.memory=800M
    spark.kubernetes.driver.volumes.persistentVolumeClaim.log.options.claimName=spark-dev-log
    spark.kubernetes.container.image=registry-vpc.cn-hangzhou.aliyuncs.com/eigenlab/data_pipeline\:test_tee
    spark.master=k8s\://https\://api.k8s.aipp.io\:6443
    spark.history.kerberos.keytab=none
    spark.driver.memory=1000M
    spark.kubernetes.driver.pod.name=data-pipeline-test-1569411807621-driver
    spark.driver.host=data-pipeline-test-1569411807621-driver-svc.spark-dev.svc
    spark.kubernetes.driver.volumes.persistentVolumeClaim.log.mount.path=/tmp/log
    spark.history.ui.port=18081
    spark.driver.extraLibraryPath=/usr/hdp/current/hadoop-client/lib/native\:/usr/hdp/current/hadoop-client/lib/native/Linux-amd64-64
    spark.shuffle.io.preferDirectBufs=false
    spark.submit.deployMode=cluster
    spark.executor.extraJavaOptions=-XX\:MaxDirectMemorySize\=512m
    spark.kubernetes.authenticate.driver.serviceAccountName=spark
    spark.kubernetes.executor.podNamePrefix=data-pipeline-test-1569411807621
    spark.history.fs.logDirectory=hdfs\:///spark2-history/
    spark.kubernetes.submitInDriver=true
    spark.kubernetes.pyspark.pythonVersion=3
    spark.kubernetes.memoryOverheadFactor=0.4
    spark.app.name=data-pipeline-test
    spark.eventLog.enabled=true
    spark.kubernetes.container.image.pullPolicy=Always
    spark.kubernetes.executor.volumes.persistentVolumeClaim.log.mount.readOnly=false
    spark.driver.blockManager.port=7079
    spark.history.ui.maxApplications=500
    spark.executor.memoryOverhead=1536m
    spark.app.id=spark-b5bf94ba63bc4d1380a414eb759ea99b
    spark.sql.warehouse.dir=hdfs\:///apps/hive/warehouse/
    spark.kubernetes.python.pyFiles=
    spark.eventLog.dir=hdfs\:///spark2-history/
    spark.kubernetes.container.image.pullSecrets=aliyun-registry
    spark.driver.extraClassPath=file\:///etc/spark2/conf
    spark.history.kerberos.principal=none
    spark.kubernetes.namespace=spark-dev
    spark.executor.instances=1
    spark.jars=hdfs\://emr2-header-1.ipa.aidigger.com\:8020/spark_jars/etlsdk/prod/mysql.jar
kind: ConfigMap
metadata:
  creationTimestamp: "2019-09-25T11:43:29Z"
  name: data-pipeline-test-1569411807621-driver-conf-map
  namespace: spark-dev
  ownerReferences:
  - apiVersion: v1
    controller: true
    kind: Pod
    name: data-pipeline-test-1569411807621-driver
    uid: b1a1eec3-df89-11e9-96d8-00163f009e2e
  resourceVersion: "172170859"
  selfLink: /api/v1/namespaces/spark-dev/configmaps/data-pipeline-test-1569411807621-driver-conf-map
  uid: b1e18a14-df89-11e9-96d8-00163f009e2e
