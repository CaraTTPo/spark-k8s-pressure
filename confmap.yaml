apiVersion: v1
data:
  spark.properties: |
    #Java properties built from Kubernetes config map with name: datapipeline-1568084099332-driver-conf-map
    #Tue Sep 10 10:55:00 CST 2019
    spark.driver.port=7078
    spark.kubernetes.resource.type=python
    spark.executor.extraLibraryPath=/usr/hdp/current/hadoop-client/lib/native\:/usr/hdp/current/hadoop-client/lib/native/Linux-amd64-64
    spark.kubernetes.python.mainAppResource=hdfs\:///user/hadoop/etlsdk/run.py
    spark.ui.enabled=false
    spark.files=hdfs\:///user/hadoop/etlsdk/run.py,file\:///opt/spark/work-dir/data_pipeline.zip,file\:///opt/spark/work-dir/etlsdk.zip
    spark.history.provider=org.apache.spark.deploy.history.FsHistoryProvider
    spark.executor.memory=500M
    spark.kubernetes.container.image=registry-vpc.cn-hangzhou.aliyuncs.com/eigenlab/data_pipeline\:test
    spark.master=k8s\://https\://test-cluster.master.k8s.aipp.io\:6443
    spark.history.kerberos.keytab=none
    spark.driver.memory=1200M
    spark.kubernetes.driver.pod.name=datapipeline-1568084099332-driver
    spark.driver.host=datapipeline-1568084099332-driver-svc.dev.svc
    spark.history.ui.port=18081
    spark.driver.extraLibraryPath=/usr/hdp/current/hadoop-client/lib/native\:/usr/hdp/current/hadoop-client/lib/native/Linux-amd64-64
    spark.shuffle.io.preferDirectBufs=false
    spark.submit.deployMode=cluster
    spark.executor.extraJavaOptions=-XX\:MaxDirectMemorySize\=512m
    spark.kubernetes.authenticate.driver.serviceAccountName=spark
    spark.kubernetes.executor.podNamePrefix=datapipeline-1568084099332
    spark.history.fs.logDirectory=hdfs\:///spark2-history/
    spark.kubernetes.submitInDriver=true
    spark.kubernetes.pyspark.pythonVersion=3
    spark.kubernetes.memoryOverheadFactor=0.4
    spark.app.name=data_pipeline
    spark.eventLog.enabled=true
    spark.kubernetes.container.image.pullPolicy=Always
    spark.driver.blockManager.port=7079
    spark.executor.memoryOverhead=1536m
    spark.app.id=spark-appid-test1
    spark.sql.warehouse.dir=hdfs\:///apps/hive/warehouse/
    spark.kubernetes.python.pyFiles=file\:///opt/spark/work-dir/data_pipeline.zip,file\:///opt/spark/work-dir/etlsdk.zip
    spark.eventLog.dir=hdfs\:///spark2-history/
    spark.kubernetes.container.image.pullSecrets=aliyun-registry
    spark.driver.extraClassPath=file\:///etc/spark2/conf
    spark.history.kerberos.principal=none
    spark.yarn.queue=default
    spark.kubernetes.namespace=dev
    spark.executor.instances=1
    spark.jars=hdfs\://emr2-header-1.ipa.aidigger.com\:8020/spark_jars/etlsdk/prod/mysql.jar
    spark.yarn.historyServer.address=emr2-header-2.ipa.aidigger.com\:18081
kind: ConfigMap
metadata:
  creationTimestamp: 2019-09-10T02:55:01Z
  name: datapipeline-1568084099332-driver-conf-map
  namespace: dev
  ownerReferences:
  - apiVersion: v1
    controller: true
    kind: Pod
    name: datapipeline-1568084099332-driver
    uid: 6181cc76-d376-11e9-a205-00163e20c49c
  resourceVersion: "1542048"
  selfLink: /api/v1/namespaces/dev/configmaps/datapipeline-1568084099332-driver-conf-map
